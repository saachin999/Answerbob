{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"e724226f-5c71-439f-ad3e-50b3c44bf9f2","showTitle":false,"title":""}},"source":["<h1>Access the S3 data directly without mounting the S3 bucket</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c7267700-3087-4a16-a826-c75367d2e142","showTitle":false,"title":""}},"outputs":[],"source":["access_key = 'xxx'\n","secret_key = 'xxx'\n","sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n","sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n","\n","# If you are using Auto Loader file notification mode to load files, provide the AWS Region ID.\n","aws_region = \"ap-south-1\"\n","sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.\" + aws_region + \".amazonaws.com\")\n","\n","df = spark.read.csv('s3a://my-s3-bucket-name/foldername/*',inferSchema=True,header=True)\n","#Example#\n","#df = spark.read.csv('s3a://techykuntal-demo-data-bucket/employee/*',inferSchema=True,header=True)\n","df.show()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0b08d5eb-840e-4538-8988-6e3f1755d5dc","showTitle":false,"title":""}},"source":["<h1>Access the S3 data by mounting the S3 bucket in Databricks</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d543e70c-d907-43e3-9476-1d3c99ac8e5b","showTitle":false,"title":""}},"outputs":[],"source":["access_key = 'xxx'\n","secret_key = 'xxx'\n","encoded_secret_key = secret_key.replace(\"/\", \"%2F\")\n","aws_bucket_name = \"my-s3-bucket-name\"\n","mount_name = \"s3dataread\"\n","\n","dbutils.fs.mount(f\"s3a://{access_key}:{encoded_secret_key}@{aws_bucket_name}\", f\"/mnt/{mount_name}\")\n","display(dbutils.fs.ls(f\"/mnt/{mount_name}\"))\n","\n","file_location = \"dbfs:/mnt/s3dataread/foldername/*\"\n","#Example#\n","#file_location = \"dbfs:/mnt/s3dataread/employee/*\"\n","\n","# The applied options are for CSV files. For other file types, these will be ignored.\n","df = spark.read.csv(file_location, inferSchema = True, header= True)\n","df.printSchema()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"183c1864-eac6-4dcb-990c-a2e73397148e","showTitle":false,"title":""}},"source":["<h1>Unmount the bucket in Databricks File System</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0416a1d6-e2c9-4eb2-9ab1-43ad822a8348","showTitle":false,"title":""}},"outputs":[],"source":["dbutils.fs.unmount(f\"/mnt/s3dataread\")\n","# OR #\n","# dbutils.fs.unmount(f\"/mnt/{mount_name}\")"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":2877119539946312,"dataframes":["_sqldf"]},"pythonIndentUnit":2},"notebookName":"AWS-S3-Access-in-DataBricks","notebookOrigID":2877119539946310,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
